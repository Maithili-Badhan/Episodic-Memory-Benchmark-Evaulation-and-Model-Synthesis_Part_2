{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Path to your local repo\n",
    "git_repo_filepath = '/content/episodic-memory-benchmark'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading books (unchanged — uses existing generation wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "from epbench.src.generation.benchmark_generation_wrapper import BenchmarkGenerationWrapper\n",
    "\n",
    "book_parameters = {'indexing': 'default', 'nb_summaries': 0}\n",
    "data_folder = Path('/content/episodic-memory-benchmark/epbench/data')\n",
    "env_file = Path('/content/episodic-memory-benchmark/.env')\n",
    "\n",
    "# Generation with Claude -- 20 events (kept as-is so benchmark generation is preserved)\n",
    "prompt_parameters = {'nb_events': 20, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_default_20 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)\n",
    "\n",
    "# Generation with Claude -- 200 events\n",
    "prompt_parameters = {'nb_events': 200, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_default_200 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)\n",
    "\n",
    "print('Books loaded (generation wrapper created).')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: short name mapping (recognizes Qwen)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_short_name_from_model_name(answering_model_name, answering_kind, answering_embedding_chunk):\n",
    "    # lightweight mapper to create short display names\n",
    "    if 'gpt-4o-mini' in answering_model_name:\n",
    "        model_name = 'gpt-4o-mini'\n",
    "    elif 'gpt-4o' in answering_model_name:\n",
    "        model_name = 'gpt-4o'\n",
    "    elif 'claude-3-5-sonnet' in answering_model_name:\n",
    "        model_name = 'cl-3.5-sonnet'\n",
    "    elif 'claude-3-haiku' in answering_model_name:\n",
    "        model_name = 'cl-3-haiku'\n",
    "    elif 'o1-mini' in answering_model_name:\n",
    "        model_name = 'o1-mini'\n",
    "    elif 'llama-3.1' in answering_model_name:\n",
    "        model_name = 'llama-3.1'\n",
    "    elif 'qwen3-vl' in answering_model_name or 'qwen/qwen3' in answering_model_name:\n",
    "        model_name = 'qwen3-vl'\n",
    "    else:\n",
    "        model_name = answering_model_name.split('/')[-1]\n",
    "    \n",
    "    if answering_kind == 'prompting':\n",
    "        output = model_name\n",
    "    elif answering_kind == 'rag':\n",
    "        if answering_embedding_chunk == 'chapter':\n",
    "            output = f\"{model_name} (rag, c)\"\n",
    "        else:\n",
    "            output = f\"{model_name} (rag)\"\n",
    "    elif answering_kind == 'ftuning':\n",
    "        output = f\"{model_name} (ftuning)\"\n",
    "    else:\n",
    "        output = model_name\n",
    "\n",
    "    return output\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments configuration — replaced models with Qwen\n",
    "This list drives `get_precomputed_results` below. I replaced the previous multi-provider list with entries using `qwen/qwen3-vl-8b-thinking`. Keep ftuning entries as placeholders (most hosted OpenRouter endpoints do not support provider-side fine-tuning — you must enable only if you have that feature)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from epbench.src.evaluation.precomputed_results import get_precomputed_results\n",
    "\n",
    "experiments = [\n",
    "    # prompting, 20 events\n",
    "    {'book_nb_events': 20,  'answering_kind': 'prompting', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking'},\n",
    "    # prompting, 200 events\n",
    "    {'book_nb_events': 200, 'answering_kind': 'prompting', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking'},\n",
    "    # RAG, paragraph + chapter (placeholders)\n",
    "    {'book_nb_events': 20,  'answering_kind': 'rag', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking', 'answering_embedding_chunk': 'paragraph'},\n",
    "    {'book_nb_events': 20,  'answering_kind': 'rag', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking', 'answering_embedding_chunk': 'chapter'},\n",
    "    {'book_nb_events': 200, 'answering_kind': 'rag', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking', 'answering_embedding_chunk': 'paragraph'},\n",
    "    {'book_nb_events': 200, 'answering_kind': 'rag', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking', 'answering_embedding_chunk': 'chapter'},\n",
    "    # fine-tuning placeholders (do NOT enable unless your provider supports hosted fine-tuning and you have credentials)\n",
    "    {'book_nb_events': 20,  'answering_kind': 'ftuning', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking'},\n",
    "    {'book_nb_events': 200, 'answering_kind': 'ftuning', 'answering_model_name': 'qwen/qwen3-vl-8b-thinking'},\n",
    "]\n",
    "\n",
    "for i in range(len(experiments)):\n",
    "    if 'answering_embedding_chunk' not in experiments[i]:\n",
    "        experiments[i]['answering_embedding_chunk'] = 'n/a'\n",
    "    # keep book model name consistent (this is used by some scripts expecting a 'book_model_name')\n",
    "    experiments[i]['book_model_name'] = 'claude-3-5-sonnet-20240620'\n",
    "\n",
    "print(f\"Configured {len(experiments)} experiments (Qwen entries).\")\n",
    "\n",
    "all_benchmarks = {\n",
    "    'benchmark_claude_default_20': benchmark_claude_default_20,\n",
    "    'benchmark_claude_default_200': benchmark_claude_default_200\n",
    "}\n",
    "\n",
    "# Run precomputed-results helper (this will create EvaluationWrapper objects for each experiment)\n",
    "df = get_precomputed_results(experiments, env_file, data_folder, all_benchmarks)\n",
    "df  # show dataframe object\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to run answering loops directly (prompting / rag / ftuning) using EvaluationWrapper\n",
    "Below are example cells you can run instead of `get_precomputed_results` if you want more direct control. They call `EvaluationWrapper` for the Qwen model.\n",
    "- **Important**: make sure your `env_file` contains correct API keys / provider configs. For OpenRouter-style endpoints you likely need `OPENROUTER_API_KEY` or similar in that `.env` and the underlying `epbench` code must support that provider.\n",
    "- Fine-tuning (`ftuning`) is left as a placeholder and uses `ftuning_need_actual_tune=False` so it won't attempt hosted tuning unless you set those flags."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from epbench.src.evaluation.evaluation_wrapper import EvaluationWrapper\n",
    "from epbench.src.evaluation.generator_answers_2_rag import get_top_n\n",
    "\n",
    "# --- Prompting (in-context) ---\n",
    "for my_benchmark in [benchmark_claude_default_20, benchmark_claude_default_200]:\n",
    "    model_name = 'qwen/qwen3-vl-8b-thinking'\n",
    "    answering_parameters = {\n",
    "        'kind': 'prompting',\n",
    "        'model_name': model_name,\n",
    "        'max_new_tokens': 4096,\n",
    "        'sleeping_time': 1,\n",
    "        'policy': 'remove_duplicates'\n",
    "    }\n",
    "    print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with prompting using {model_name}\")\n",
    "    my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print('Prompting experiments finished (Qwen).')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- RAG experiments (builds on embeddings + retrieval) ---\n",
    "for my_benchmark in [benchmark_claude_default_20, benchmark_claude_default_200]:\n",
    "    for embedding_chunk in ['paragraph', 'chapter']:\n",
    "        model_name = 'qwen/qwen3-vl-8b-thinking'\n",
    "        answering_parameters = {\n",
    "            'kind': 'rag',\n",
    "            'model_name': model_name,\n",
    "            'embedding_chunk': embedding_chunk,\n",
    "            'max_new_tokens': 4096,\n",
    "            'sleeping_time': 0,\n",
    "            'embedding_model': 'text-embedding-3-small',\n",
    "            'embedding_batch_size': 2048,\n",
    "            'top_n': get_top_n(embedding_chunk, my_benchmark),\n",
    "            'policy': 'remove_duplicates'\n",
    "        }\n",
    "        print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with rag using {model_name} ({embedding_chunk})\")\n",
    "        my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print('RAG experiments finished (Qwen).')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Fine-tuning placeholders ---\n",
    "for my_benchmark in [benchmark_claude_default_20, benchmark_claude_default_200]:\n",
    "    model_name = 'qwen/qwen3-vl-8b-thinking'\n",
    "    answering_parameters = {\n",
    "        'kind': 'ftuning',\n",
    "        'model_name': model_name,\n",
    "        'max_new_tokens': 4096,\n",
    "        'sleeping_time': 0,\n",
    "        # The code below will NOT actually perform hosted tuning unless you enable ftuning_need_actual_tune=True\n",
    "        'ftuning_input_data_policy': 'single',\n",
    "        'ftuning_need_upload': False,\n",
    "        'ftuning_need_actual_tune': False,\n",
    "        'batch_size': 'auto',\n",
    "        'learning_rate_multiplier': 'auto',\n",
    "        'n_epochs': 5,\n",
    "        'policy': 'remove_duplicates'\n",
    "    }\n",
    "    print(f\"(Placeholder) Document with {my_benchmark.nb_tokens()} tokens, ftuning using {model_name} — ftuning disabled by default\")\n",
    "    my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print('Ftuning placeholders executed (no hosted tuning by default).')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis / plotting cells (kept as in original notebook)\n",
    "Run these after `df` (the dataframe built earlier) is available."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from epbench.src.results.average_groups import extract_groups\n",
    "nb_events = 200\n",
    "relative_to = ['get', 'bins_items_correct_answer']\n",
    "df_results = extract_groups(df, nb_events, relative_to)\n",
    "df_results = df_results[df_results['get'] == 'all'].drop('get', axis = 1)\n",
    "df_results.head()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes / next steps\n",
    "- If the notebook UI still shows old content after you edited the raw JSON, **restart kernel and clear outputs**, then re-run the notebook top→bottom.\n",
    "- For OpenRouter / open-source-hosted Qwen endpoints: ensure your `.env` contains the proper variables expected by the `epbench` code (check `epbench` auth/provider wrappers). Many community endpoints require different env var names; inspect `epbench` provider code if you need to change key names.\n",
    "- Fine-tuning: provider dependent. Most public endpoints (OpenRouter) do not offer the same hosted fine-tuning mechanics as OpenAI; treat ftuning entries as placeholders unless you have a provider-specific API and the `epbench` wrapper supports it.\n"
   ]
  }
 ]
}